{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will develop a simple LSTM network to learn sequences of characters from Alice in Wonderland. In the next section we will use this model to generate new sequences of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s start off by importing the classes and functions we intend to use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the vocabulary that the network must learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the book is loaded, we must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers.\n",
    "\n",
    "We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the list of unique sorted lowercase characters in the book is as follows:\n",
    "\n",
    "['\\n', '\\r', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xbb', '\\xbf', '\\xef']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there may be some characters that we could remove to further clean up the dataset that will reduce the vocabulary and may improve the modeling process.\n",
    "\n",
    "Now that the book has been loaded and the mapping prepared, we can summarize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163781\n",
      "Total Vocab:  59\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code to this point produces the following output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  163681\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1279/1279 [==============================] - 465s 362ms/step - loss: 3.0701\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.97561, saving model to weights-improvement-01-2.9756.hdf5\n",
      "Epoch 2/20\n",
      "1279/1279 [==============================] - 496s 388ms/step - loss: 2.8278\n",
      "\n",
      "Epoch 00002: loss improved from 2.97561 to 2.79720, saving model to weights-improvement-02-2.7972.hdf5\n",
      "Epoch 3/20\n",
      "1279/1279 [==============================] - 492s 385ms/step - loss: 2.7333\n",
      "\n",
      "Epoch 00003: loss improved from 2.79720 to 2.71812, saving model to weights-improvement-03-2.7181.hdf5\n",
      "Epoch 4/20\n",
      "1279/1279 [==============================] - 476s 372ms/step - loss: 2.6665\n",
      "\n",
      "Epoch 00004: loss improved from 2.71812 to 2.65105, saving model to weights-improvement-04-2.6510.hdf5\n",
      "Epoch 5/20\n",
      "1279/1279 [==============================] - 498s 389ms/step - loss: 2.6073\n",
      "\n",
      "Epoch 00005: loss improved from 2.65105 to 2.59369, saving model to weights-improvement-05-2.5937.hdf5\n",
      "Epoch 6/20\n",
      "1279/1279 [==============================] - 496s 388ms/step - loss: 2.5493\n",
      "\n",
      "Epoch 00006: loss improved from 2.59369 to 2.53871, saving model to weights-improvement-06-2.5387.hdf5\n",
      "Epoch 7/20\n",
      "1279/1279 [==============================] - 488s 382ms/step - loss: 2.4965\n",
      "\n",
      "Epoch 00007: loss improved from 2.53871 to 2.48771, saving model to weights-improvement-07-2.4877.hdf5\n",
      "Epoch 8/20\n",
      "1279/1279 [==============================] - 475s 371ms/step - loss: 2.4448\n",
      "\n",
      "Epoch 00008: loss improved from 2.48771 to 2.44494, saving model to weights-improvement-08-2.4449.hdf5\n",
      "Epoch 9/20\n",
      "1279/1279 [==============================] - 571s 446ms/step - loss: 2.4062\n",
      "\n",
      "Epoch 00009: loss improved from 2.44494 to 2.40247, saving model to weights-improvement-09-2.4025.hdf5\n",
      "Epoch 10/20\n",
      "1279/1279 [==============================] - 579s 452ms/step - loss: 2.3674\n",
      "\n",
      "Epoch 00010: loss improved from 2.40247 to 2.35837, saving model to weights-improvement-10-2.3584.hdf5\n",
      "Epoch 11/20\n",
      "1279/1279 [==============================] - 584s 457ms/step - loss: 2.3175\n",
      "\n",
      "Epoch 00011: loss improved from 2.35837 to 2.32101, saving model to weights-improvement-11-2.3210.hdf5\n",
      "Epoch 12/20\n",
      "1279/1279 [==============================] - 565s 441ms/step - loss: 2.2855\n",
      "\n",
      "Epoch 00012: loss improved from 2.32101 to 2.28177, saving model to weights-improvement-12-2.2818.hdf5\n",
      "Epoch 13/20\n",
      "1279/1279 [==============================] - 476s 372ms/step - loss: 2.2523\n",
      "\n",
      "Epoch 00013: loss improved from 2.28177 to 2.24601, saving model to weights-improvement-13-2.2460.hdf5\n",
      "Epoch 14/20\n",
      "1279/1279 [==============================] - 493s 385ms/step - loss: 2.2075\n",
      "\n",
      "Epoch 00014: loss improved from 2.24601 to 2.21359, saving model to weights-improvement-14-2.2136.hdf5\n",
      "Epoch 15/20\n",
      "1279/1279 [==============================] - 461s 360ms/step - loss: 2.1820\n",
      "\n",
      "Epoch 00015: loss improved from 2.21359 to 2.18110, saving model to weights-improvement-15-2.1811.hdf5\n",
      "Epoch 16/20\n",
      "1279/1279 [==============================] - 422s 330ms/step - loss: 2.1435\n",
      "\n",
      "Epoch 00016: loss improved from 2.18110 to 2.15081, saving model to weights-improvement-16-2.1508.hdf5\n",
      "Epoch 17/20\n",
      "1279/1279 [==============================] - 418s 327ms/step - loss: 2.1137\n",
      "\n",
      "Epoch 00017: loss improved from 2.15081 to 2.12155, saving model to weights-improvement-17-2.1215.hdf5\n",
      "Epoch 18/20\n",
      "1279/1279 [==============================] - 465s 364ms/step - loss: 2.0874\n",
      "\n",
      "Epoch 00018: loss improved from 2.12155 to 2.09389, saving model to weights-improvement-18-2.0939.hdf5\n",
      "Epoch 19/20\n",
      "1279/1279 [==============================] - 495s 387ms/step - loss: 2.0598\n",
      "\n",
      "Epoch 00019: loss improved from 2.09389 to 2.06816, saving model to weights-improvement-19-2.0682.hdf5\n",
      "Epoch 20/20\n",
      "1279/1279 [==============================] - 483s 377ms/step - loss: 2.0344\n",
      "\n",
      "Epoch 00020: loss improved from 2.06816 to 2.04315, saving model to weights-improvement-20-2.0432.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1775f3e2b20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
